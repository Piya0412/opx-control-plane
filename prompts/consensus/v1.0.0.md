# Consensus & Confidence Agent - Logic Specification v1.0.0

**Agent ID:** `consensus`  
**Version:** v1.0.0  
**Date:** 2026-01-25  
**Implementation:** LangGraph Node (Python)  
**Schema Version:** 2026-01  

---

## System Overview

The Consensus & Confidence Agent is a **LangGraph node** (NOT a Bedrock Agent) that performs deterministic aggregation and conflict resolution on agent outputs. It does not use LLM reasoning.

### Why LangGraph Node (Not Bedrock Agent)?

1. **Deterministic computation** - Pure math, no natural language generation
2. **Cost reduction** - No LLM invocation required
3. **Replay safety** - Exact reproducibility guaranteed
4. **Performance** - No network latency from Bedrock API calls

### Critical Constraints

1. **Deterministic logic ONLY** - No LLM calls, no randomness
2. **Weighted aggregation** - Agent weights based on historical performance
3. **Conflict resolution** - Highest confidence wins, preserve minority opinions
4. **Quality metrics** - Data completeness, citation quality, reasoning coherence

---

## Input Format

```python
@dataclass
class ConsensusInput:
    incident_id: str
    agent_outputs: Dict[AgentId, AgentOutput]  # All prior agent outputs
    agent_weights: Dict[AgentId, float]  # Historical performance weights
    timestamp: str
```

---

## Output Format

```python
@dataclass
class ConsensusOutput:
    agent_id: str = "consensus"
    agent_version: str = "1.0.0"
    execution_id: str
    timestamp: str
    duration: int  # milliseconds
    status: ExecutionStatus
    confidence: float  # Aggregated confidence (0.0-1.0)
    reasoning: str  # Explanation of consensus
    disclaimer: str = "HYPOTHESIS_ONLY_NOT_AUTHORITATIVE"
    
    findings: ConsensusFindings
    cost: CostMetadata  # Zero cost (no LLM)
    replay_metadata: ReplayMetadata
```

---

## Logic Specification

### Step 1: Aggregate Confidence

```python
def aggregate_confidence(
    agent_outputs: Dict[AgentId, AgentOutput],
    agent_weights: Dict[AgentId, float]
) -> float:
    """
    Compute weighted average confidence across all agents.
    
    Formula:
        aggregated_confidence = Σ(agent_confidence * agent_weight) / Σ(agent_weight)
    
    Args:
        agent_outputs: All agent outputs with confidence scores
        agent_weights: Historical performance weights (0.0-1.0)
    
    Returns:
        Aggregated confidence (0.0-1.0)
    """
    total_weighted_confidence = 0.0
    total_weight = 0.0
    
    for agent_id, output in agent_outputs.items():
        weight = agent_weights.get(agent_id, 0.5)  # Default weight 0.5
        total_weighted_confidence += output.confidence * weight
        total_weight += weight
    
    return total_weighted_confidence / total_weight if total_weight > 0 else 0.0
```

### Step 2: Compute Agreement Level

```python
def compute_agreement_level(
    agent_outputs: Dict[AgentId, AgentOutput]
) -> float:
    """
    Measure consensus across agents using confidence variance.
    
    Formula:
        agreement_level = 1.0 - (std_dev(confidences) / max_possible_std_dev)
    
    Args:
        agent_outputs: All agent outputs with confidence scores
    
    Returns:
        Agreement level (0.0-1.0)
        - 1.0 = perfect agreement (all same confidence)
        - 0.0 = maximum disagreement (confidences at extremes)
    """
    confidences = [output.confidence for output in agent_outputs.values()]
    
    if len(confidences) < 2:
        return 1.0  # Single agent = perfect agreement
    
    mean_confidence = sum(confidences) / len(confidences)
    variance = sum((c - mean_confidence) ** 2 for c in confidences) / len(confidences)
    std_dev = variance ** 0.5
    
    # Max possible std_dev is 0.5 (when confidences are at 0.0 and 1.0)
    max_std_dev = 0.5
    
    return 1.0 - (std_dev / max_std_dev)
```

### Step 3: Detect Conflicts

```python
def detect_conflicts(
    agent_outputs: Dict[AgentId, AgentOutput],
    confidence_threshold: float = 0.3
) -> List[Conflict]:
    """
    Identify conflicting recommendations across agents.
    
    Conflict criteria:
        - Agents recommend different actions (INVESTIGATION vs ROLLBACK)
        - Confidence difference > threshold
        - Contradicting evidence cited
    
    Args:
        agent_outputs: All agent outputs
        confidence_threshold: Min confidence difference to flag conflict
    
    Returns:
        List of detected conflicts
    """
    conflicts = []
    
    # Group recommendations by type
    recommendations_by_type = defaultdict(list)
    for agent_id, output in agent_outputs.items():
        for rec in output.recommendations:
            recommendations_by_type[rec.type].append((agent_id, rec))
    
    # Detect conflicts: same type, different descriptions, high confidence difference
    for rec_type, recs in recommendations_by_type.items():
        for i, (agent_id_1, rec_1) in enumerate(recs):
            for agent_id_2, rec_2 in recs[i+1:]:
                confidence_diff = abs(rec_1.confidence - rec_2.confidence)
                
                if confidence_diff > confidence_threshold:
                    conflicts.append(Conflict(
                        agents=[agent_id_1, agent_id_2],
                        conflict_type="CONFIDENCE_DIVERGENCE",
                        resolution=f"Higher confidence wins: {agent_id_1 if rec_1.confidence > rec_2.confidence else agent_id_2}"
                    ))
    
    return conflicts
```

### Step 4: Resolve Conflicts

```python
def resolve_conflicts(
    conflicts: List[Conflict],
    agent_outputs: Dict[AgentId, AgentOutput]
) -> Dict[str, Any]:
    """
    Resolve conflicts using highest confidence wins strategy.
    
    Resolution rules:
        1. Highest confidence recommendation wins
        2. Preserve minority opinions (do not discard)
        3. Explain resolution rationale
    
    Args:
        conflicts: Detected conflicts
        agent_outputs: All agent outputs
    
    Returns:
        Resolved recommendations with rationale
    """
    resolved = {}
    
    for conflict in conflicts:
        # Find highest confidence agent
        highest_confidence_agent = max(
            conflict.agents,
            key=lambda agent_id: agent_outputs[agent_id].confidence
        )
        
        resolved[conflict.conflict_type] = {
            "winner": highest_confidence_agent,
            "confidence": agent_outputs[highest_confidence_agent].confidence,
            "rationale": f"Highest confidence among conflicting agents"
        }
    
    return resolved
```

### Step 5: Synthesize Unified Recommendation

```python
def synthesize_unified_recommendation(
    agent_outputs: Dict[AgentId, AgentOutput],
    resolved_conflicts: Dict[str, Any]
) -> str:
    """
    Create unified recommendation from all agent outputs.
    
    Synthesis rules:
        1. Prioritize high-confidence recommendations
        2. Group by recommendation type (INVESTIGATION, MITIGATION, ROLLBACK)
        3. Note consensus level
        4. Preserve minority opinions
    
    Args:
        agent_outputs: All agent outputs
        resolved_conflicts: Resolved conflicts
    
    Returns:
        Unified recommendation string
    """
    # Group recommendations by type and confidence
    recommendations = defaultdict(list)
    for agent_id, output in agent_outputs.items():
        for rec in output.recommendations:
            recommendations[rec.type].append((agent_id, rec))
    
    # Sort by confidence (descending)
    for rec_type in recommendations:
        recommendations[rec_type].sort(key=lambda x: x[1].confidence, reverse=True)
    
    # Build unified recommendation
    unified = []
    for rec_type, recs in recommendations.items():
        top_rec = recs[0]  # Highest confidence
        unified.append(f"{rec_type}: {top_rec[1].description} (confidence: {top_rec[1].confidence:.2f})")
    
    return " | ".join(unified)
```

### Step 6: Compute Quality Metrics

```python
def compute_quality_metrics(
    agent_outputs: Dict[AgentId, AgentOutput]
) -> QualityMetrics:
    """
    Assess overall quality of agent outputs.
    
    Metrics:
        - data_completeness: % of agents that returned SUCCESS status
        - citation_quality: % of agents with citations
        - reasoning_coherence: Agreement level (from step 2)
    
    Args:
        agent_outputs: All agent outputs
    
    Returns:
        Quality metrics (0.0-1.0 for each)
    """
    total_agents = len(agent_outputs)
    
    # Data completeness
    success_count = sum(1 for output in agent_outputs.values() if output.status == "SUCCESS")
    data_completeness = success_count / total_agents if total_agents > 0 else 0.0
    
    # Citation quality
    citation_count = sum(1 for output in agent_outputs.values() if output.citations)
    citation_quality = citation_count / total_agents if total_agents > 0 else 0.0
    
    # Reasoning coherence (use agreement level from step 2)
    reasoning_coherence = compute_agreement_level(agent_outputs)
    
    return QualityMetrics(
        data_completeness=data_completeness,
        citation_quality=citation_quality,
        reasoning_coherence=reasoning_coherence
    )
```

---

## Complete Implementation

```python
def consensus_agent(state: AgentState) -> AgentState:
    """
    LangGraph node for consensus and confidence aggregation.
    
    Args:
        state: Current LangGraph state with agent outputs
    
    Returns:
        Updated state with consensus findings
    """
    start_time = time.time()
    
    agent_outputs = state["agent_outputs"]
    agent_weights = get_agent_weights()  # From historical performance
    
    # Step 1: Aggregate confidence
    aggregated_confidence = aggregate_confidence(agent_outputs, agent_weights)
    
    # Step 2: Compute agreement level
    agreement_level = compute_agreement_level(agent_outputs)
    
    # Step 3: Detect conflicts
    conflicts = detect_conflicts(agent_outputs)
    
    # Step 4: Resolve conflicts
    resolved = resolve_conflicts(conflicts, agent_outputs)
    
    # Step 5: Synthesize unified recommendation
    unified_recommendation = synthesize_unified_recommendation(agent_outputs, resolved)
    
    # Step 6: Compute quality metrics
    quality_metrics = compute_quality_metrics(agent_outputs)
    
    # Build consensus output
    consensus_output = ConsensusOutput(
        execution_id=state["execution_id"],
        timestamp=datetime.utcnow().isoformat(),
        duration=int((time.time() - start_time) * 1000),
        status="SUCCESS",
        confidence=aggregated_confidence,
        reasoning=f"Consensus from {len(agent_outputs)} agents with {agreement_level:.2f} agreement level",
        findings=ConsensusFindings(
            aggregated_confidence=aggregated_confidence,
            agreement_level=agreement_level,
            conflicts_detected=conflicts,
            unified_recommendation=unified_recommendation,
            minority_opinions=extract_minority_opinions(agent_outputs, resolved),
            quality_metrics=quality_metrics
        ),
        cost=CostMetadata(
            input_tokens=0,
            output_tokens=0,
            estimated_cost=0.0,
            model="N/A"
        ),
        replay_metadata=ReplayMetadata(
            deterministic_hash=compute_deterministic_hash(state, aggregated_confidence),
            schema_version="1.0.0"
        )
    )
    
    # Update state
    state["agent_outputs"]["consensus"] = consensus_output
    return state
```

---

## Validation Rules

1. **Determinism:** Same inputs MUST produce same outputs (no randomness)
2. **Confidence bounds:** Aggregated confidence MUST be in range [0.0, 1.0]
3. **Agreement level:** MUST be in range [0.0, 1.0]
4. **Quality metrics:** All metrics MUST be in range [0.0, 1.0]
5. **Zero cost:** Cost MUST be 0.0 (no LLM invocation)

---

## Testing Requirements

### Unit Tests

- Test weighted confidence aggregation with various weights
- Test agreement level calculation with edge cases (single agent, identical confidences, extreme divergence)
- Test conflict detection with various confidence thresholds
- Test conflict resolution (highest confidence wins)
- Test quality metrics calculation

### Integration Tests

- Test with real agent outputs from Phase 6
- Verify deterministic hash matches on replay
- Verify zero cost (no Bedrock API calls)
- Verify execution time <10 seconds

---

## Disclaimer

**IMPORTANT:** This agent performs **DETERMINISTIC AGGREGATION ONLY**. It does not use LLM reasoning. It combines agent outputs using mathematical formulas and rule-based logic.

---

**Version:** v1.0.0  
**Status:** Production  
**Last Updated:** 2026-01-25
